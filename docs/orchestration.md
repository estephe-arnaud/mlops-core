# Orchestration ‚Äî Kubernetes

Ce document explique **comment d√©ployer et faire tourner l‚ÄôAPI ML sur un cluster Kubernetes** (local ou cloud). Vous y trouverez : les **concepts K8s** utiles (Pods, Deployments, Services, volumes), l‚Äô**architecture** du projet (API, MLflow, Job d‚Äôentra√Ænement), un **guide de d√©ploiement** pas √† pas, les **trois workflows MLflow** (mod√®le local, entra√Ænement en cluster, GCS), ainsi que l‚Äô**auto-scaling**, les **tests** et le **d√©pannage**.

## üß≠ Navigation

| ‚Üê Pr√©c√©dent | Suivant ‚Üí |
|-------------|-----------|
| [Exp√©rimentation](experimentation.md) | [Observabilit√©](observability.md) |
| [Retour au README](../README.md) | [Documentation](README.md) |

## üìã Table des Mati√®res

1. [Objectif](#-objectif)
2. [T√¢ches √† Accomplir](#-t√¢ches-√†-accomplir)
3. [Livrables Cr√©√©s](#-livrables-cr√©√©s)
4. [Fonctionnalit√©s Impl√©ment√©es](#-fonctionnalit√©s-impl√©ment√©es)
5. [Concepts Kubernetes](#-concepts-kubernetes)
6. [Architecture du D√©ploiement](#-architecture-du-d√©ploiement)
7. [Installation et Configuration](#-installation-et-configuration)
8. [Guide de D√©ploiement](#-guide-de-d√©ploiement)
9. [Workflows MLflow](#-workflows-mlflow)
10. [Auto-Scaling avec HPA](#-auto-scaling-avec-hpa)
11. [Tests et Validation](#-tests-et-validation)
12. [Commandes Utiles](#-commandes-utiles)
13. [S√©curit√©](#-s√©curit√©)
14. [D√©pannage](#-d√©pannage)
15. [M√©triques](#-m√©triques)
16. [Validation des Objectifs](#-validation-des-objectifs)
17. [Prochaines √©tapes](#-prochaines-√©tapes-observabilit√©)
18. [Ressources](#-ressources)

---

## üéØ Objectif

**Orchestrer l‚Äôapplication ML containeris√©e sur Kubernetes avec haute disponibilit√© et auto-scaling.**

√Ä la fin de ce parcours, vous saurez d√©ployer l‚ÄôAPI FastAPI et le serveur MLflow sur un cluster (minikube/kind ou cloud), g√©rer la configuration et les secrets, et faire √©voluer le nombre de pods selon la charge.

### ‚ùì Questions auxquelles ce document r√©pond

- Qu‚Äôest-ce qu‚Äôun Pod, un Deployment et un Service dans Kubernetes ?
- Comment exposer une application dockeris√©e dans un cluster K8s ?
- Comment g√©rer les configurations et secrets dans Kubernetes ?
- Comment mettre en place le scaling automatique bas√© sur les m√©triques ?

### ‚è±Ô∏è R√©partition indicative (20 h)

| Phase | Dur√©e | Contenu |
|-------|--------|---------|
| Concepts K8s | 8 h | Pods, Deployments, Services, ConfigMaps, Secrets, Namespaces |
| Installation | 8 h | kubectl, minikube ou kind, cluster local |
| D√©ploiement | 4 h | Manifests, API + MLflow, health checks, exposition |

---

## üìã T√¢ches √† Accomplir

Les t√¢ches ci-dessous correspondent au parcours type : de l‚Äôapprentissage des concepts √† la mise en production sur le cluster.

### 1. üéì Apprendre les Concepts Kubernetes
- Comprendre l'architecture d'un cluster Kubernetes
- Ma√Ætriser les concepts de base : Pods, Deployments, Services
- G√©rer les configurations avec ConfigMaps et Secrets
- Comprendre les Namespaces pour l'isolation

### 2. üõ†Ô∏è Installation et Configuration
- Installer kubectl (client Kubernetes)
- Configurer un cluster local (minikube ou kind)
- V√©rifier la connectivit√© au cluster

### 3. üöÄ D√©ploiement de l'Application
- Cr√©er les manifests Kubernetes (Deployment, Service, ConfigMap, Secret)
- D√©ployer l'API FastAPI sur le cluster
- Configurer les health checks (liveness et readiness probes)
- Exposer l'API via Service et Ingress

### 4. üìä Int√©gration MLflow
- D√©ployer un serveur MLflow dans le cluster
- Configurer le partage de volumes pour les donn√©es MLflow
- Connecter l'API au serveur MLflow

### 5. ‚öñÔ∏è Auto-Scaling
- Configurer le Horizontal Pod Autoscaler (HPA)
- D√©finir les m√©triques de scaling (CPU, m√©moire)
- Tester le scaling automatique

---

## üì¶ Livrables Cr√©√©s

Cette section liste les **fichiers et ressources Kubernetes** produits par le projet. Ils permettent de d√©ployer l‚ÄôAPI, le serveur MLflow, le job d‚Äôentra√Ænement et l‚Äôauto-scaling.

### Structure des Fichiers Kubernetes

```
k8s/
‚îú‚îÄ‚îÄ namespace.yaml              # Namespace mlops pour isolation
‚îú‚îÄ‚îÄ deployment.yaml             # Deployment API (2 replicas)
‚îú‚îÄ‚îÄ mlflow-deployment.yaml      # Deployment MLflow server (1 replica)
‚îú‚îÄ‚îÄ mlflow-pvc.yaml             # PVC pour les runs MLflow (/app/mlruns)
‚îú‚îÄ‚îÄ models-pvc.yaml             # PVC pour les mod√®les et m√©tadonn√©es (/app/models)
‚îú‚îÄ‚îÄ train-job.yaml              # Job Kubernetes pour entra√Æner le mod√®le et mettre √† jour /app/models
‚îú‚îÄ‚îÄ service.yaml                # Service ClusterIP pour l'API
‚îú‚îÄ‚îÄ mlflow-service.yaml         # Service ClusterIP pour MLflow
‚îú‚îÄ‚îÄ service-nodeport.yaml       # Service NodePort (dev/test)
‚îú‚îÄ‚îÄ configmap.yaml              # Configuration non sensible
‚îú‚îÄ‚îÄ secret.yaml.example         # Template pour secrets
‚îú‚îÄ‚îÄ ingress.yaml                # Ingress pour exposition externe
‚îú‚îÄ‚îÄ hpa.yaml                    # Horizontal Pod Autoscaler
‚îî‚îÄ‚îÄ README.md                   # Guide rapide de d√©ploiement
```

### Fichiers Principaux

#### `k8s/deployment.yaml` - D√©ploiement de l'API
- **Replicas** : 2 pods pour haute disponibilit√©
- **Strategy** : RollingUpdate (zero-downtime)
- **Health Checks** : Liveness et readiness probes sur `/health`
- **Ressources** : Requests et limits CPU/m√©moire
- **S√©curit√©** : Containers non-root, capabilities limit√©es
- **Volumes** :
  - `mlruns-volume` (hostPath) pour le mode MLflow local (d√©veloppement)
  - `models-volume` (PVC `models-pvc`) pour les fichiers de mod√®le (`/app/models`)

#### `k8s/mlflow-deployment.yaml` - Serveur MLflow
- **Replicas** : 1 (singleton)
- **Strategy** : Recreate (serveur avec √©tat)
- **Image** : `ghcr.io/mlflow/mlflow:v2.9.2`
- **Backend Store** : Fichier local (`file:///app/mlruns`)
- **Volume** : PVC d√©di√© `mlflow-pvc` pour les runs MLflow (params, metrics, tags). Avec artifact root `file://`, les artifacts ne sont pas enregistr√©s sur le serveur depuis un client distant ; pour le serving, le job √©crit `model.joblib` dans le PVC `models-pvc`.

#### `k8s/train-job.yaml` - Job d'Entra√Ænement dans le Cluster
- **Kind** : `Job` (Batch)
- **Image** : `iris-api:latest` (r√©utilise le code d'entra√Ænement existant)
- **R√¥le** :
  - Entra√Æne le mod√®le dans le cluster
  - Loggue le run dans MLflow (`MLFLOW_TRACKING_URI` = `mlflow-server-service`)
  - √âcrit `model.joblib`, `metadata.json` et `metrics.json` dans `/app/models` (PVC `models-pvc`)

#### `k8s/service.yaml` - Service ClusterIP
- **Type** : ClusterIP (acc√®s interne uniquement)
- **Port** : 8000
- **Selector** : `app: iris-api`
- **Load Balancing** : Round-robin entre les pods

#### `k8s/configmap.yaml` - Configuration
- Variables d'environnement non sensibles :
  - `ENVIRONMENT`: production
  - `MODEL_DIR`: /app/models
  - `LOG_LEVEL`: INFO

#### `k8s/secret.yaml.example` - Template Secrets
- `API_KEY`: Cl√© API pour authentification
- `MLFLOW_TRACKING_URI`: URI du serveur MLflow ou GCS

#### `k8s/hpa.yaml` - Auto-Scaling
- **Min replicas** : 2
- **Max replicas** : 10
- **M√©triques** : CPU (70%) et m√©moire (80%)
- **Comportement** : Scaling up r√©actif, scaling down prudent

#### `k8s/ingress.yaml` - Exposition Externe
- **Controller** : nginx-ingress
- **TLS** : Support HTTPS (cert-manager)
- **Annotations** : Rate limiting, CORS, timeouts

**En r√©sum√©** : L‚ÄôAPI et MLflow sont d√©ploy√©s via des Deployments ; le **Job** `iris-train-job` entra√Æne le mod√®le et √©crit dans le PVC `models-pvc` ; l‚ÄôAPI charge le mod√®le depuis `/app/models`. Les ConfigMaps et Secrets fournissent la configuration et les cl√©s.

---

## ‚úÖ Fonctionnalit√©s Impl√©ment√©es

Cette section r√©capitule **ce qui est d√©j√† en place** dans le projet : d√©ploiement, services, configuration, MLflow, HPA et commandes Makefile.

### D√©ploiement Kubernetes
- ‚úÖ Namespace `mlops` pour isolation
- ‚úÖ Deployment avec 2 replicas pour haute disponibilit√©
- ‚úÖ Rolling update sans interruption de service
- ‚úÖ Health checks (liveness et readiness probes)
- ‚úÖ Gestion des ressources (requests et limits)
- ‚úÖ S√©curit√© renforc√©e (non-root, capabilities limit√©es)

### Services et Exposition
- ‚úÖ Service ClusterIP pour acc√®s interne
- ‚úÖ Service NodePort pour d√©veloppement/test
- ‚úÖ Ingress pour exposition externe avec TLS
- ‚úÖ Load balancing automatique entre pods

### Configuration et Secrets
- ‚úÖ ConfigMap pour variables d'environnement non sensibles
- ‚úÖ Secrets Kubernetes pour donn√©es sensibles (API keys)
- ‚úÖ Injection via `envFrom` et `env`
- ‚úÖ Template de secret avec instructions

### MLflow Integration
- ‚úÖ Serveur MLflow d√©ploy√© dans le cluster
- ‚úÖ Partage de volumes entre API et MLflow
- ‚úÖ Service ClusterIP pour acc√®s interne
- ‚úÖ Support de trois modes :
  - Serveur MLflow dans K8s (recommand√©)
  - Local avec hostPath (d√©veloppement)
  - GCS (production cloud)

> üîç **Note d‚Äôarchitecture**  
> Dans ce projet, MLflow est utilis√© comme **source de v√©rit√© analytique** (UI, runs, Model Registry),
> tandis que le **runtime de l‚ÄôAPI** consomme une copie contr√¥l√©e du mod√®le via un PVC (`/app/models`).
> Cela √©vite de d√©pendre d‚Äôimpl√©mentations parfois ambigu√´s de `mlruns/.../artifacts` avec un backend
> `file:///` et un serveur HTTP, tout en restant tr√®s proche d‚Äôun setup r√©el (Job d‚Äôentra√Ænement ‚Üí PVC
> ‚Üí API). En contexte entreprise, la v2 naturelle serait : **backend SQL + object store (S3/MinIO/GCS) +
> chargement `models:/name/stage` c√¥t√© API**.

### Auto-scaling
- ‚úÖ Horizontal Pod Autoscaler (HPA) configur√©
- ‚úÖ Scaling bas√© sur CPU et m√©moire
- ‚úÖ Comportement configurable (stabilisation, politiques)
- ‚úÖ M√©triques via metrics-server

### Commandes Makefile
- ‚úÖ `make k8s-setup` : Installation minikube/kind
- ‚úÖ `make k8s-deploy` : D√©ploiement API (avec PVC, sans serveur MLflow)
- ‚úÖ `make k8s-deploy-mlflow` : D√©ploiement API + MLflow server (avec PVC)
- ‚úÖ `make k8s-status` : V√©rification du statut
- ‚úÖ `make k8s-logs` : Visualisation des logs
- ‚úÖ `make k8s-port-forward` : Acc√®s √† l'API
- ‚úÖ `make k8s-mlflow-ui` : Acc√®s √† MLflow UI
- ‚úÖ `make k8s-test` : Tests automatis√©s
- ‚úÖ `make k8s-clean` : Nettoyage complet

---

## üéì Concepts Kubernetes

Avant de d√©ployer, il est utile de comprendre les **concepts de base** utilis√©s dans ce projet. Chaque concept est illustr√© par son usage concret (API, MLflow, Job).

### Pod

**Plus petite unit√© d√©ployable** dans Kubernetes. Un Pod contient un ou plusieurs containers qui partagent :
- Le m√™me r√©seau (m√™me IP)
- Le m√™me stockage (volumes)
- Le m√™me namespace

**Exemple** : Un Pod contient l'API FastAPI.

### Deployment
**Orchestrateur qui g√®re un ensemble de Pods identiques** (replicas). Assure :
- ‚úÖ Cr√©ation et mise √† jour des Pods
- ‚úÖ Rolling update (d√©ploiement sans interruption)
- ‚úÖ Rollback en cas de probl√®me
- ‚úÖ Scaling (augmentation/r√©duction)

**Dans notre cas** : 2 Pods identiques pour la haute disponibilit√©.

### Service
**Expose un ensemble de Pods comme un service r√©seau**. Fournit :
- ‚úÖ IP stable (ClusterIP)
- ‚úÖ √âquilibrage de charge entre les Pods
- ‚úÖ DNS interne (`service-name.namespace.svc.cluster.local`)

**Types** :
- **ClusterIP** : Acc√®s interne uniquement
- **NodePort** : Acc√®s externe via port sur chaque node
- **LoadBalancer** : IP publique externe (cloud)
- **Ingress** : Routage HTTP/HTTPS bas√© sur domaine

### ConfigMap
**Stocke des donn√©es de configuration non sensibles** (cl√©-valeur).

**Dans notre cas** : `ENVIRONMENT`, `MODEL_DIR`, `LOG_LEVEL`.

### Secret
**Stocke des donn√©es sensibles** (cl√©s API, mots de passe). Similaire √† ConfigMap mais :
- ‚úÖ Encod√© en base64
- ‚úÖ Plus s√©curis√© (ne pas exposer dans les logs)

**Dans notre cas** : `API_KEY`, `MLFLOW_TRACKING_URI`.

### Namespace
**Isole des ressources dans un cluster**. Utile pour :
- ‚úÖ S√©parer les environnements (dev, staging, prod)
- ‚úÖ Limiter les permissions (RBAC)
- ‚úÖ Organiser les ressources

**Dans notre cas** : Namespace `mlops` pour toutes les ressources.

### Volume
**Permet aux pods de partager des donn√©es**. Types :
- **hostPath** : Monte un r√©pertoire de la machine h√¥te (surtout pour le dev local)
- **PersistentVolume / PersistentVolumeClaim (PVC)** : Stockage persistant g√©r√© par Kubernetes
- **ConfigMap/Secret** : Mont√©s comme volumes

**Dans notre cas** :
- Un **PVC d√©di√©** (`mlflow-pvc`) pour stocker les runs MLflow dans `/app/mlruns` (mode serveur K8s)
- Un **hostPath + minikube mount** uniquement pour le mode local (d√©veloppement)

### HPA (Horizontal Pod Autoscaler)

**Ajuste automatiquement le nombre de replicas** selon les m√©triques (CPU, m√©moire). Kubernetes compare l‚Äôutilisation actuelle aux seuils d√©finis et ajoute ou retire des pods.

**Dans notre cas** : Scale entre 2 et 10 pods selon CPU (70 %) et m√©moire (80 %).

**√Ä retenir** : Pod = unit√© de base ; Deployment = orchestrateur de Pods identiques ; Service = point d‚Äôacc√®s r√©seau stable ; ConfigMap/Secret = configuration ; Namespace = isolation ; Volume/PVC = stockage partag√© ; HPA = scaling automatique.

---

## üèóÔ∏è Architecture du D√©ploiement

Cette section d√©crit **comment les composants sont organis√©s** dans le cluster et comment ils communiquent (r√©seau, volumes, DNS).

### Vue d‚Äôensemble

Le cluster h√©berge **deux applications m√©tier** dans le namespace `mlops` (API et MLflow), et **optionnellement** un Ingress Controller dans un autre namespace pour exposer l‚ÄôAPI vers l‚Äôext√©rieur.

| Application | Namespace | R√¥le |
|-------------|-----------|------|
| **iris-api** (FastAPI) | `mlops` | API ML pour pr√©dictions (2 replicas) |
| **mlflow-server** (MLflow) | `mlops` | Tracking des runs (params, metrics, tags) |
| **nginx** (Ingress Controller) | `ingress-nginx` | Optionnel : reverse proxy, routage HTTP/HTTPS |

### Namespaces

Le projet utilise le namespace **`mlops`** pour l‚ÄôAPI et MLflow ; un namespace **`ingress-nginx`** (ou √©quivalent) est optionnel pour l‚ÄôIngress Controller.

#### Namespace `ingress-nginx`

**R√¥le** : H√©berge l'Ingress Controller nginx (optionnel, pour exposition externe)

**Ressources** :
- Deployment `ingress-nginx-controller`
- Service `ingress-nginx-controller` (LoadBalancer ou NodePort)
- ConfigMaps, Secrets pour la configuration nginx

**Installation** :
```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```

**V√©rification** :
```bash
kubectl get pods -n ingress-nginx
kubectl get service -n ingress-nginx
```

#### Namespace `mlops`

**R√¥le** : H√©berge les applications m√©tier (API et MLflow)

**Ressources** :
- Deployment `iris-api` (2 replicas)
- Deployment `mlflow-server` (1 replica)
- Services `iris-api-service` et `mlflow-server-service`
- ConfigMap `iris-api-config`
- Secret `iris-api-secrets`
- Ingress `iris-api-ingress` (optionnel)
- HPA `iris-api-hpa` (optionnel)

**Cr√©ation** :
```bash
kubectl apply -f k8s/namespace.yaml
```

**V√©rification** :
```bash
kubectl get all -n mlops
```

### Applications et pods

Chaque application est d√©ploy√©e via un **Deployment** (ou √©quivalent) et expos√©e via un **Service**.

#### 1. Nginx Ingress Controller (optionnel)

**Namespace** : `ingress-nginx` (ou `kube-system`)

**Deployment** : `ingress-nginx-controller`

**Container** :
- **Image** : `registry.k8s.io/ingress-nginx/controller`
- **Application** : nginx (reverse proxy)
- **Ports** : 80 (HTTP), 443 (HTTPS)

**R√¥le** :
- ‚úÖ Lit les r√®gles Ingress de tous les namespaces
- ‚úÖ Route le trafic HTTP/HTTPS vers les Services appropri√©s
- ‚úÖ G√®re TLS/HTTPS (terminaison SSL)
- ‚úÖ Rate limiting (protection DDoS)
- ‚úÖ CORS (Cross-Origin Resource Sharing)
- ‚úÖ Load balancing au niveau HTTP

**Service** :
- **Type** : `LoadBalancer` (production cloud) ou `NodePort` (local)
- **Acc√®s** : Production via IP publique du LoadBalancer, Local via `http://<node-ip>:<nodePort>`

#### 2. Iris API (FastAPI)

**Namespace** : `mlops`

**Deployment** : `iris-api`

**Pods** : `iris-api-<hash>-1`, `iris-api-<hash>-2` (2 replicas)

**Container** :
- **Image** : `iris-api:latest` (ou depuis Artifact Registry)
- **Application** : FastAPI (serveur web Python)
- **Port** : 8000

**R√¥le** :
- ‚úÖ API REST pour pr√©dictions ML
- ‚úÖ Endpoints : `/predict`, `/health`, `/metrics`
- ‚úÖ Authentification via API Key
- ‚úÖ Charge les mod√®les depuis MLflow
- ‚úÖ M√©triques Prometheus

**Service** :
- **Type** : `ClusterIP` (acc√®s interne uniquement)
- **DNS** : `iris-api-service.mlops.svc.cluster.local`
- **Port** : 8000

**Acc√®s** :
- Depuis nginx : `http://iris-api-service:8000`
- Depuis mlflow-server : `http://iris-api-service:8000`
- Depuis l'ext√©rieur : Via port-forward ou Ingress

#### 3. MLflow Server

**Namespace** : `mlops`

**Deployment** : `mlflow-server`

**Pod** : `mlflow-server-<hash>` (1 replica)

**Container** :
- **Image** : `ghcr.io/mlflow/mlflow:v2.9.2`
- **Application** : MLflow (serveur de tracking ML)
- **Port** : 5000

**R√¥le** :
- ‚úÖ Stocke les runs ML (exp√©riences, param√®tres, m√©triques, tags)
- ‚úÖ UI MLflow (interface web) et API REST
- ‚úÖ Avec backend `file://`, les artifacts ne sont pas stock√©s sur le serveur depuis un client distant ; le serving du mod√®le repose sur le PVC `models-pvc` (voir [Workflows MLflow](#-workflows-mlflow))

**Service** :
- **Type** : `ClusterIP` (acc√®s interne uniquement)
- **DNS** : `mlflow-server-service.mlops.svc.cluster.local`
- **Port** : 5000

**Acc√®s** :
- Depuis iris-api : `http://mlflow-server-service:5000`
- Depuis l'ext√©rieur : Via port-forward (`make k8s-mlflow-ui`)

### Services

Chaque application est expos√©e via un **Service** (ClusterIP dans ce projet) pour un acc√®s r√©seau stable et un load balancing entre les pods.

#### Service iris-api

**Namespace** : `mlops`

**Nom** : `iris-api-service`

**Type** : `ClusterIP` (interne uniquement)

**Port** : 8000 ‚Üí 8000

**S√©lecteur** : `app: iris-api`

**DNS** : `iris-api-service.mlops.svc.cluster.local`

**R√¥le** :
- ‚úÖ Load balancing entre les 2 pods iris-api
- ‚úÖ DNS stable (m√™me si les pods red√©marrent)
- ‚úÖ Point d'acc√®s unique pour nginx

**Acc√®s depuis nginx** :
```yaml
# Dans ingress.yaml
backend:
  service:
    name: iris-api-service  # Service dans namespace mlops
    port:
      number: 8000
```

#### Service mlflow-server

**Namespace** : `mlops`

**Nom** : `mlflow-server-service`

**Type** : `ClusterIP` (interne uniquement)

**Port** : 5000 ‚Üí 5000

**S√©lecteur** : `app: mlflow-server`

**DNS** : `mlflow-server-service.mlops.svc.cluster.local`

**R√¥le** :
- ‚úÖ Point d'acc√®s stable pour mlflow-server
- ‚úÖ Utilis√© par iris-api pour charger les mod√®les

**Acc√®s depuis iris-api** :
```python
# Dans le code Python
MLFLOW_TRACKING_URI = "http://mlflow-server-service:5000"
```

### Communication inter-namespace

Kubernetes permet la communication entre namespaces via le **DNS interne**. Chaque Service a une adresse DNS stable.

#### Format DNS Kubernetes

```
<service-name>.<namespace>.svc.cluster.local
```

#### Exemples dans l'Architecture

**1. Nginx ‚Üí Iris API** :
```yaml
# Dans ingress.yaml (namespace: mlops)
# Nginx (namespace: ingress-nginx) lit cette r√®gle
backend:
  service:
    name: iris-api-service  # Service dans namespace mlops
    port:
      number: 8000
```

**DNS utilis√©** : `iris-api-service.mlops.svc.cluster.local:8000`

**2. Iris API ‚Üí MLflow Server** :
```python
# Dans secret.yaml (namespace: mlops)
MLFLOW_TRACKING_URI: "http://mlflow-server-service:5000"
# ou explicitement :
# MLFLOW_TRACKING_URI: "http://mlflow-server-service.mlops.svc.cluster.local:5000"
```

**DNS utilis√©** : `mlflow-server-service.mlops.svc.cluster.local:5000`

#### Raccourci DNS

Dans le m√™me namespace, vous pouvez utiliser juste le nom du service :

```python
# Dans namespace mlops
MLFLOW_TRACKING_URI: "http://mlflow-server-service:5000"
# √âquivalent √† :
# MLFLOW_TRACKING_URI: "http://mlflow-server-service.mlops.svc.cluster.local:5000"
```

### Volumes partag√©s

Les pods ont besoin de **stockage persistant** pour les runs MLflow et pour les fichiers de mod√®le utilis√©s par l‚ÄôAPI. Cette section d√©crit les volumes utilis√©s.

#### Volume `mlruns-volume` (mode serveur K8s)

**Type** : `PersistentVolumeClaim`

**PVC** : `mlflow-pvc`

**Mont√© dans** :

**1. Pod mlflow-server** :
```yaml
volumeMounts:
- name: mlruns-volume
  mountPath: /app/mlruns  # MLflow stocke tout ici
  readOnly: false
```

**Usage** :
- ‚úÖ Toujours n√©cessaire en mode serveur K8s (MLflow stocke les runs dans `/app/mlruns`)

**2. Pods iris-api (optionnel)** :

Si l'API devait acc√©der aux artifacts MLflow (par ex. avec un artifact store partag√© ou en mode local avec hostPath), elle pourrait monter le m√™me PVC. Dans le setup actuel, l'API charge le mod√®le depuis `/app/models` (PVC `models-pvc`) o√π le job √©crit `model.joblib`.

```yaml
volumeMounts:
- name: mlruns-volume
  mountPath: /app/mlruns  # Acc√®s en lecture/√©criture aux artifacts MLflow (si partag√©s)
  readOnly: false
```

#### Mode Local avec `hostPath` (D√©veloppement uniquement)

En mode local (sans serveur MLflow dans K8s), on peut continuer √† utiliser un `hostPath` + `minikube mount` :

```bash
minikube mount $(pwd)/mlruns:/tmp/mlruns
```

Puis monter `/tmp/mlruns` dans les pods :

```yaml
volumes:
- name: mlruns-volume
  hostPath:
    path: /tmp/mlruns
    type: DirectoryOrCreate

volumeMounts:
- name: mlruns-volume
  mountPath: /app/mlruns
  readOnly: false
```

**Usage** :
- ‚úÖ Utile pour exp√©rimenter rapidement en local
- ‚ùå √Ä √©viter en production (pr√©f√©rer un PVC ou un backend objet type GCS/S3)

#### Partage de donn√©es

**Avec serveur MLflow dans K8s** : Le serveur MLflow stocke les runs dans `/app/mlruns` (PVC `mlflow-pvc`). L‚ÄôAPI charge le mod√®le depuis `/app/models` (PVC `models-pvc`), pas depuis les artifacts MLflow lorsque le backend est `file://`. Le volume `mlruns` est donc utilis√© par le serveur MLflow ; le volume `models` est utilis√© par l‚ÄôAPI et le Job d‚Äôentra√Ænement.

**Mode local (hostPath)** : Le r√©pertoire `mlruns/` de la machine est mont√© dans le cluster ; l‚ÄôAPI charge le mod√®le depuis `/app/mlruns` (metadata + run MLflow).

### Flux de trafic

Comment les requ√™tes circulent entre le client, l‚ÄôAPI et MLflow (Ingress, interne, port-forward).

#### Flux 1 : Client ‚Üí API (via Ingress)

**√âtapes** :
1. Client Internet envoie une requ√™te HTTP/HTTPS vers `iris-api.example.com`
2. DNS r√©sout vers l'IP du LoadBalancer (nginx)
3. Service `ingress-nginx-controller` route vers le Pod nginx (namespace: `ingress-nginx`)
4. Nginx lit les r√®gles Ingress (cherche dans TOUS les namespaces)
5. Nginx trouve l'Ingress `iris-api-ingress` (namespace: `mlops`)
6. Nginx route vers le Service `iris-api-service` (namespace: `mlops`)
7. Service load balance vers un Pod iris-api (1 ou 2)
8. FastAPI traite la requ√™te et retourne la r√©ponse

#### Flux 2 : API ‚Üí chargement du mod√®le

**Comportement actuel** : L‚ÄôAPI charge en priorit√© le mod√®le depuis le fichier local `/app/models/model.joblib` (PVC `models-pvc`). Si ce fichier n‚Äôexiste pas, elle peut interroger le serveur MLflow (`http://mlflow-server-service:5000`) pour r√©cup√©rer les m√©tadonn√©es ou le mod√®le (par ex. avec un backend GCS). Avec un backend `file://` sur MLflow, les artifacts ne sont pas sur le serveur ; le flux de serving repose donc sur le PVC `models-pvc`.

#### Flux 3 : Port-Forward (d√©veloppement)

**√âtapes** :
1. Votre machine locale utilise `kubectl port-forward`
2. Le port-forward se connecte directement au Service `iris-api-service`
3. Service load balance vers un Pod iris-api (1 ou 2)
4. FastAPI traite la requ√™te et retourne la r√©ponse sur `localhost:8000`

**Note** : Le port-forward contourne compl√®tement nginx et l'Ingress.

### Modes MLflow

Selon la valeur de `MLFLOW_TRACKING_URI`, l‚ÄôAPI et le Job utilisent un mode diff√©rent (serveur K8s, local, GCS).

| Mode | MLFLOW_TRACKING_URI | Volume | Usage |
|------|---------------------|--------|-------|
| **K8s Server** | `http://mlflow-server-service:5000` | PVC `mlflow-pvc` (mont√© sur `/app/mlruns`) | Portfolio/Production |
| **Local** | `""` | hostPath + `minikube mount` | D√©veloppement |
| **GCS** | `gs://bucket/mlruns/` | Aucun | Production cloud |

### Tableau r√©capitulatif

| Composant | Namespace | Type | Nom | Port | Acc√®s |
|-----------|-----------|------|-----|------|-------|
| **nginx** | `ingress-nginx` | Deployment | `ingress-nginx-controller` | 80, 443 | Internet (LoadBalancer) |
| **iris-api** | `mlops` | Deployment | `iris-api` | 8000 | Interne (ClusterIP) |
| **mlflow-server** | `mlops` | Deployment | `mlflow-server` | 5000 | Interne (ClusterIP) |
| **Ingress** | `mlops` | Ingress | `iris-api-ingress` | - | R√®gles de routage |
| **Volume** | `mlops` | Volume | `mlruns-volume` / `models-volume` | - | Partag√© (MLflow runs / mod√®les API) |

**En r√©sum√©** : L‚ÄôAPI (`iris-api`) et MLflow (`mlflow-server`) tournent dans `mlops`. L‚ÄôAPI appelle MLflow via le service `mlflow-server-service`. L‚ÄôAPI charge le mod√®le depuis le PVC `models-pvc` (`/app/models`), pas depuis les artifacts MLflow lorsque le backend est `file://`. L‚ÄôIngress (optionnel) expose l‚ÄôAPI vers l‚Äôext√©rieur.

---

## üöÄ Installation et Configuration

Cette section d√©crit **comment installer les outils** (kubectl, minikube ou kind) et cr√©er un cluster local utilisable pour le reste du guide.

### Pr√©requis

| Outil | Version | Description |
|-------|---------|-------------|
| **kubectl** | >= 1.28 | Client Kubernetes |
| **Docker** | >= 20.10 | Pour minikube/kind |
| **minikube** ou **kind** | >= 1.30 / >= 0.20 | Cluster local (un des deux suffit) |

### Installation Automatique (Recommand√©)

```bash
# Avec minikube
make k8s-setup

# Avec kind
make k8s-setup-kind

# Ou directement
./scripts/setup-k8s.sh minikube
./scripts/setup-k8s.sh kind
```

### Installation Manuelle

#### 1. Installer kubectl

**macOS** :
```bash
brew install kubectl
```

**Linux** :
```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

#### 2. Installer minikube ou kind

**minikube** (macOS) :
```bash
brew install minikube
minikube start --driver=docker --memory=4096 --cpus=2
```

**kind** :
```bash
brew install kind  # macOS
kind create cluster --name mlops-cluster
```

#### 3. V√©rifier

```bash
kubectl cluster-info
kubectl get nodes
```

---

## üöÄ Guide de D√©ploiement

Ce guide d√©crit **l‚Äôordre recommand√©** pour un premier d√©ploiement : image Docker, secrets, application des manifests, v√©rification et acc√®s √† l‚ÄôAPI et √† MLflow.

### √âtape 1 : Pr√©parer l‚Äôimage Docker

**Option A : Image Locale (minikube)**
```bash
eval $(minikube docker-env)
make build
```

**Option B : Artifact Registry (Production)**
```yaml
# Dans k8s/deployment.yaml
image: europe-west1-docker.pkg.dev/PROJECT_ID/mlops-repo/iris-api:latest
imagePullPolicy: Always
```

### √âtape 2 : Pr√©parer les Secrets

```bash
cp k8s/secret.yaml.example k8s/secret.yaml
# √âditer k8s/secret.yaml avec vos valeurs
# ‚ö†Ô∏è Ne jamais commiter secret.yaml dans le d√©p√¥t !
```

**Contenu de `k8s/secret.yaml`** :
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: iris-api-secrets
  namespace: mlops
type: Opaque
stringData:
  API_KEY: "votre-api-key-ici"  # openssl rand -hex 32
  MLFLOW_TRACKING_URI: "http://mlflow-server-service:5000"  # Ou "gs://bucket/mlruns/"
```

### √âtape 3 : D√©ployer

**Option A : Avec MLflow Server** (Recommand√©)
```bash
make k8s-deploy-mlflow
```

**Option B : MLflow Local**
```bash
# 1. Monter mlruns/ (terminal s√©par√©)
minikube mount $(pwd)/mlruns:/tmp/mlruns

# 2. D√©ployer
make k8s-deploy
```

**D√©ploiement manuel (sans make)** : pour reproduire **`make k8s-deploy-mlflow`** (API + MLflow), appliquer dans l‚Äôordre :
```bash
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml
kubectl apply -f k8s/mlflow-pvc.yaml
kubectl apply -f k8s/models-pvc.yaml
kubectl apply -f k8s/mlflow-deployment.yaml
kubectl apply -f k8s/mlflow-service.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

### √âtape 4 : V√©rifier le d√©ploiement

```bash
make k8s-status
# ou
kubectl get pods,services -n mlops
```

**R√©sultat attendu** :
```
NAME                        READY   STATUS    RESTARTS   AGE
iris-api-xxxxxxxxxx-xxxxx   1/1     Running   0          30s
iris-api-xxxxxxxxxx-xxxxx   1/1     Running   0          30s
mlflow-server-xxxxx         1/1     Running   0          30s
```

### √âtape 5 : Acc√©der √† l‚ÄôAPI et √† MLflow

**Port-Forward** (D√©veloppement) :
```bash
make k8s-port-forward
# http://localhost:8000
```

**MLflow UI** (si d√©ploy√©) :
```bash
make k8s-mlflow-ui
# http://localhost:5000
```

**NodePort** (Test) :
```bash
kubectl apply -f k8s/service-nodeport.yaml
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
curl http://$NODE_IP:30080/health
```

**Ingress** (production) :
```bash
# Installer Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
kubectl apply -f k8s/ingress.yaml
```

**En r√©sum√©** : Image ‚Üí Secret ‚Üí `make k8s-deploy-mlflow` (ou `k8s-deploy`) ‚Üí v√©rifier les pods ‚Üí acc√©der via port-forward, NodePort ou Ingress. Voir [k8s/README.md](../k8s/README.md) pour un guide pas √† pas d√©taill√©.

---

## üîÑ Workflows MLflow

**Trois fa√ßons** d‚Äôutiliser MLflow et l‚ÄôAPI sur Kubernetes, selon la **source du mod√®le** (d√©j√† entra√Æn√© en local, entra√Ænement dans le cluster, backend cloud) et l‚Äô**environnement** (d√©veloppement, cluster local, production GCP). Choisir le workflow adapt√© √† votre cas.

---

### Workflow 1 : Utiliser un mod√®le d√©j√† entra√Æn√© (local ‚Üí cluster)

**Quand l‚Äôutiliser** : Vous avez d√©j√† entra√Æn√© un mod√®le en local ; les runs sont dans `mlruns/`. Vous voulez servir ce mod√®le depuis l‚ÄôAPI d√©ploy√©e sur le cluster sans serveur MLflow dans K8s.

**Id√©e** : Monter le r√©pertoire `mlruns/` de votre machine vers le cluster (minikube), d√©ployer l‚ÄôAPI avec un volume hostPath pointant vers ce montage. L‚ÄôAPI charge le mod√®le depuis `/app/mlruns` (metadata + run MLflow).

| √âtape | Action |
|-------|--------|
| 1 | Dans un **terminal d√©di√©** (laisser tourner) : `minikube mount $(pwd)/mlruns:/tmp/mlruns` |
| 2 | Dans le secret K8s : `MLFLOW_TRACKING_URI: ""` (ou chemin local). Puis d√©ployer l‚ÄôAPI : `make k8s-deploy` |
| 3 | V√©rifier : `kubectl exec -it deployment/iris-api -n mlops -- ls -la /app/mlruns` |

**R√©sultat** : L‚ÄôAPI lit les runs depuis le volume mont√© ; pas de serveur MLflow dans le cluster. Adapt√© au d√©veloppement / d√©mo.

---

### Workflow 2 : Entra√Æner dans le cluster et servir depuis le PVC (recommand√©)

**Quand l‚Äôutiliser** : Vous voulez entra√Æner un nouveau mod√®le **dans le cluster**, tracker les runs dans MLflow, et servir le mod√®le via l‚ÄôAPI. C‚Äôest le flux recommand√© pour un usage ‚Äúproduction‚Äù sur un cluster local (minikube/kind).

**Id√©e** : Un **Job** Kubernetes lance l‚Äôentra√Ænement ; il √©crit `model.joblib`, `metadata.json` et `metrics.json` dans le PVC `models-pvc` (mont√© en `/app/models`). Il envoie aussi params, metrics et tags au **serveur MLflow** (tracking uniquement : avec artifact root `file://`, les artifacts ne sont pas stock√©s sur le serveur). L‚ÄôAPI monte le m√™me PVC et charge le mod√®le depuis `/app/models`. Apr√®s chaque nouvel entra√Ænement, on red√©marre l‚ÄôAPI pour qu‚Äôelle relise le PVC.

| √âtape | Action |
|-------|--------|
| 1 | D√©ployer l‚Äôinfra : namespace, PVC (`mlflow-pvc`, `models-pvc`), MLflow server, configmap, secret (`MLFLOW_TRACKING_URI="http://mlflow-server-service:5000"`), deployment et service de l‚ÄôAPI. Ex. : `make k8s-deploy-mlflow` ou appliquer les manifests dans l‚Äôordre. |
| 2 | Lancer le job d‚Äôentra√Ænement : `kubectl delete job iris-train-job -n mlops --ignore-not-found` puis `kubectl apply -f k8s/train-job.yaml` ; suivre les logs : `kubectl logs job/iris-train-job -n mlops -f`. |
| 3 | Recharger l‚ÄôAPI pour qu‚Äôelle relise `/app/models` : `kubectl rollout restart deployment/iris-api -n mlops`. |

**Option ‚Äî Entra√Ænement en local, tracking vers le serveur MLflow** : Si vous lancez `make train` en local avec `MLFLOW_TRACKING_URI="http://localhost:5000"` (apr√®s port-forward du service MLflow), seuls params, metrics et tags sont enregistr√©s sur le serveur ; les artifacts ne le sont pas. Pour servir ce mod√®le en cluster, il faut ensuite l‚Äô√©crire dans le PVC (par ex. via un job ou une copie de `model.joblib` + metadata/metrics vers le volume).

**R√©sultat** : Mod√®le servi depuis `/app/models` (PVC partag√©) ; MLflow utilis√© pour le tracking (params, metrics, tags). Voir [k8s/README.md](../k8s/README.md) pour le d√©tail des commandes.

---

### Workflow 3 : Production avec GCS (backend MLflow dans le cloud)

**Quand l‚Äôutiliser** : Environnement de production sur GCP ; le backend MLflow (runs + artifacts) est un bucket GCS. Pas de volume hostPath ni de PVC MLflow dans le cluster pour les runs.

**Id√©e** : Configurer `MLFLOW_TRACKING_URI` avec l‚ÄôURI GCS du bucket (ex. `gs://bucket-name/mlruns/`). L‚ÄôAPI et les jobs utilisent ce backend pour le tracking et le chargement du mod√®le ; pas besoin de monter `mlruns` dans les pods.

| √âtape | Action |
|-------|--------|
| 1 | Cr√©er un bucket GCS (ou utiliser un existant) et configurer dans le secret : `MLFLOW_TRACKING_URI: "gs://bucket-name/mlruns/"`. |
| 2 | D√©ployer l‚ÄôAPI (namespace, configmap, secret, PVC, deployment, service) : **`make k8s-deploy`**. Les PVC sont inclus ; pas de serveur MLflow dans le cluster. |
| 3 | L‚ÄôAPI charge le mod√®le depuis GCS via les m√©tadonn√©es (ex. `metadata.json` ou run_id) ; les artifacts sont lus depuis le bucket. |

**R√©sultat** : Runs et artifacts MLflow dans GCS ; cluster K8s sans d√©pendance √† un volume local pour MLflow.

---

## üìä Auto-Scaling avec HPA

Le **Horizontal Pod Autoscaler (HPA)** ajuste automatiquement le nombre de replicas de l‚ÄôAPI en fonction de l‚Äôutilisation CPU et m√©moire. Utile en production pour absorber les pics de charge sans surdimensionner le cluster.

### Installation de metrics-server

Le HPA s‚Äôappuie sur les m√©triques fournies par **metrics-server** (utilisation CPU/m√©moire des pods). √Ä installer une fois sur le cluster.

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### D√©ploiement du HPA

```bash
kubectl apply -f k8s/hpa.yaml
```

### V√©rification

```bash
kubectl get hpa -n mlops
kubectl describe hpa iris-api-hpa -n mlops
```

### Test du scaling

```bash
# G√©n√©rer de la charge
while true; do curl http://localhost:8000/health; done

# Observer le scaling
watch kubectl get pods -n mlops
kubectl get hpa -n mlops
```

Le HPA scale automatiquement entre 2 et 10 pods selon CPU (70 %) et m√©moire (80 %).

---

## üß™ Tests et Validation

Cette section d√©crit **comment v√©rifier** que le d√©ploiement fonctionne : health check, pr√©diction, logs, scaling.

### Test 1 : Health check

```bash
make k8s-port-forward  # Terminal 1
curl http://localhost:8000/health  # Terminal 2
```

**R√©sultat attendu** :
```json
{
  "status": "healthy",
  "model_loaded": true,
  "version": "1.0.0"
}
```

### Test 2 : Pr√©diction (endpoint `/predict`)

```bash
export API_KEY=$(kubectl get secret iris-api-secrets -n mlops -o jsonpath='{.data.API_KEY}' | base64 -d)

curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $API_KEY" \
  -d '{
    "sepal_length": 5.1,
    "sepal_width": 3.5,
    "petal_length": 1.4,
    "petal_width": 0.2
  }'
```

### Test 3 : Logs (d√©bogage)

```bash
make k8s-logs
# ou
kubectl logs -f deployment/iris-api -n mlops
```

### Test 4 : Scaling manuel

```bash
kubectl scale deployment iris-api --replicas=3 -n mlops
kubectl get pods -n mlops
```

### Test 5 : Auto-scaling (HPA)

```bash
# Installer metrics-server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# D√©ployer HPA
kubectl apply -f k8s/hpa.yaml

# G√©n√©rer de la charge
while true; do curl http://localhost:8000/health; done

# Observer le scaling
watch kubectl get pods -n mlops
```

---

## üìù Commandes utiles

Les commandes ci-dessous couvrent le **cycle de vie** du d√©ploiement : cr√©ation du cluster, d√©ploiement, statut, logs, acc√®s, tests, nettoyage.

### Commandes Makefile

| Commande | Description |
|----------|-------------|
| `make k8s-setup` | Installer minikube et cr√©er le cluster |
| `make k8s-setup-kind` | Installer kind et cr√©er le cluster |
| `make k8s-deploy` | D√©ployer l'API (avec PVC, sans serveur MLflow) |
| `make k8s-deploy-mlflow` | D√©ployer API + serveur MLflow (avec PVC) |
| `make k8s-status` | V√©rifier le statut |
| `make k8s-logs` | Voir les logs |
| `make k8s-port-forward` | Port-forward vers l'API |
| `make k8s-mlflow-ui` | Port-forward vers MLflow UI |
| `make k8s-test` | Tester l'API |
| `make k8s-clean` | Nettoyer compl√®tement |

### Commandes kubectl essentielles

```bash
# Voir toutes les ressources
kubectl get all -n mlops

# D√©crire un pod
kubectl describe pod <pod-name> -n mlops

# Ex√©cuter une commande dans un pod
kubectl exec -it <pod-name> -n mlops -- /bin/bash

# Voir les √©v√©nements
kubectl get events -n mlops --sort-by='.lastTimestamp'

# Red√©marrer le d√©ploiement
kubectl rollout restart deployment/iris-api -n mlops

# Rollback
kubectl rollout undo deployment/iris-api -n mlops

# Voir les ressources utilis√©es
kubectl top pods -n mlops
```

---

## üîí S√©curit√©

Cette section r√©sume les **bonnes pratiques** d√©j√† appliqu√©es dans les manifests et les **recommandations** pour aller plus loin en production.

### Bonnes pratiques impl√©ment√©es

- ‚úÖ **Secrets Kubernetes** : Jamais en clair dans Git
- ‚úÖ **Containers non-root** : `runAsNonRoot: true`, `runAsUser: 1000`
- ‚úÖ **Capabilities limit√©es** : `drop: [ALL]`
- ‚úÖ **Read-only root filesystem** : Optionnel (d√©sactiv√© pour logs)
- ‚úÖ **Seccomp profile** : `RuntimeDefault`
- ‚úÖ **TLS via Ingress** : Support HTTPS en production
- ‚úÖ **RBAC** : Permissions limit√©es par namespace

### Recommandations production

- üîê Utiliser External Secrets Operator avec Secret Manager GCP/AWS
- üîê Activer Network Policies pour isolation r√©seau
- üîê Configurer Pod Security Standards
- üîê Utiliser cert-manager pour TLS automatique
- üîê Activer audit logging
- üîê Scanner les images pour vuln√©rabilit√©s (Trivy, Snyk)

---

## üîç D√©pannage

En cas de probl√®me, utiliser les commandes ci-dessous pour **diagnostiquer** (pods, API, secrets, image, HPA) et corriger les causes courantes.

### Pods ne d√©marrent pas

**Sympt√¥mes** : `Pending` ou `CrashLoopBackOff`

**Solutions** :
```bash
kubectl describe pod <pod-name> -n mlops
kubectl logs <pod-name> -n mlops
kubectl get events -n mlops --sort-by='.lastTimestamp'

# Causes courantes :
# - Image non trouv√©e : V√©rifier deployment.yaml
# - Secrets manquants : V√©rifier secret.yaml
# - Ressources insuffisantes : V√©rifier le cluster
```

### API ne r√©pond pas

**Solutions** :
```bash
kubectl get pods -n mlops
kubectl logs -f deployment/iris-api -n mlops
kubectl get service iris-api-service -n mlops

# Tester depuis un pod
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- curl http://iris-api-service:8000/health
```

### Secrets non trouv√©s

**Solutions** :
```bash
kubectl get secret iris-api-secrets -n mlops
kubectl describe secret iris-api-secrets -n mlops

# Recr√©er si n√©cessaire
kubectl delete secret iris-api-secrets -n mlops
kubectl apply -f k8s/secret.yaml
```

### Image non trouv√©e

**Avec minikube** :
```bash
eval $(minikube docker-env)
docker build -t iris-api:latest .
```

**Avec Artifact Registry** :
```bash
gcloud auth configure-docker europe-west1-docker.pkg.dev
# Modifier deployment.yaml avec l'image compl√®te
```

### HPA ne fonctionne pas

**Solutions** :
```bash
# V√©rifier metrics-server
kubectl get deployment metrics-server -n kube-system

# Installer si n√©cessaire
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# V√©rifier les m√©triques
kubectl top pods -n mlops
kubectl describe hpa iris-api-hpa -n mlops
```

---

## üìä M√©triques

R√©sum√© des **ressources et capacit√©s** d√©ploy√©es par ce projet (nombre de manifests, pods, services, commandes).

| M√©trique | Valeur |
|----------|--------|
| **Fichiers cr√©√©s** | 10+ manifests Kubernetes |
| **Pods d√©ploy√©s** | 2 (API) + 1 (MLflow) |
| **Services** | 2 (ClusterIP) |
| **Auto-scaling** | 2-10 pods selon charge |
| **Health checks** | Liveness + Readiness |
| **Commandes Make** | 10+ commandes k8s-* |

---

## ‚úÖ Validation des Objectifs

Tableau de **suivi des objectifs** du parcours : chaque ligne correspond √† un objectif √† valider.

| Objectif | Status | D√©tails |
|----------|--------|---------|
| **Concepts K8s** | ‚úÖ | Compris : Pods, Deployments, Services, ConfigMaps, Secrets |
| **Installation** | ‚úÖ | minikube/kind install√© et cluster cr√©√© |
| **Manifests** | ‚úÖ | Tous les manifests cr√©√©s |
| **D√©ploiement** | ‚úÖ | API d√©ploy√©e sur le cluster local |
| **Health Checks** | ‚úÖ | Liveness et readiness probes configur√©s |
| **MLflow Integration** | ‚úÖ | Serveur MLflow d√©ploy√© et connect√© |
| **Auto-Scaling** | ‚úÖ | HPA configur√© et fonctionnel |
| **Tests** | ‚úÖ | API accessible et fonctionnelle |
| **Documentation** | ‚úÖ | Guide complet avec exemples |

---

## üöÄ Prochaines √©tapes : Observabilit√©

Une fois l‚Äôorchestration en place, la suite logique est l‚Äô**observabilit√©** : m√©triques, dashboards, alertes.

Voir [Observabilit√©](observability.md) pour :
- üìä Observabilit√© & Monitoring (Prometheus, Grafana, AlertManager)
- üîç M√©triques avanc√©es
- üìà Dashboards de monitoring
- üö® Alertes et notifications

---

## üìö Ressources

Liens utiles pour **aller plus loin** : documentation du projet, Kubernetes, minikube/kind, MLflow, HPA.

### Documentation

- [k8s/README.md](../k8s/README.md) ‚Äî D√©ploiement et workflows (MLflow / API seule)
- [Kubernetes Documentation](https://kubernetes.io/docs/) - Documentation officielle
- [minikube](https://minikube.sigs.k8s.io/) - Cluster local
- [kind](https://kind.sigs.k8s.io/) - Kubernetes in Docker

### Ressources externes

- [Kubernetes Concepts](https://kubernetes.io/docs/concepts/)
- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [MLflow Kubernetes](https://mlflow.org/docs/latest/tracking.html#scenario-5-mlflow-on-kubernetes)
- [HPA Documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)

---

**Orchestration termin√©e avec succ√®s.**

L‚ÄôAPI MLOps est maintenant d√©ploy√©e sur Kubernetes avec :
- ‚úÖ Haute disponibilit√© (2 replicas)
- ‚úÖ Health checks configur√©s
- ‚úÖ Configuration et secrets g√©r√©s
- ‚úÖ Auto-scaling optionnel (HPA)
- ‚úÖ Serveur MLflow int√©gr√©
- ‚úÖ Documentation compl√®te

Le projet est pr√™t pour l‚Äôobservabilit√© (Prometheus, Grafana, AlertManager).
