"""
Script de prÃ©paration des donnÃ©es pour le dataset Iris
Lit les paramÃ¨tres depuis params.yaml avec validation Pydantic
"""

import logging
from pathlib import Path
from typing import Optional, Tuple

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from src.config import get_config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def prepare_iris_data(
    test_size: Optional[float] = None, random_state: Optional[int] = None
) -> Tuple[Path, Path]:
    """
    PrÃ©pare le dataset Iris et le divise en train/test
    Sauvegarde les fichiers dans data/processed/

    Args:
        test_size: Proportion du dataset pour le test (surcharge params.yaml si fourni)
        random_state: Graine alÃ©atoire (surcharge params.yaml si fourni)

    Returns:
        Tuple[Path, Path]: Chemins vers les fichiers train.csv et test.csv
    """
    config = get_config()
    test_size = test_size if test_size is not None else config.data.test_size
    random_state = (
        random_state if random_state is not None else config.data.random_state
    )

    logger.info("ğŸŒ± Chargement du dataset Iris...")
    logger.info(f"   ParamÃ¨tres: test_size={test_size}, random_state={random_state}")

    iris = load_iris()

    # CrÃ©er un DataFrame
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df["target"] = iris.target
    df["target_name"] = df["target"].apply(lambda x: iris.target_names[x])

    # CrÃ©er les rÃ©pertoires
    raw_dir = Path("data/raw")
    processed_dir = Path("data/processed")
    raw_dir.mkdir(parents=True, exist_ok=True)
    processed_dir.mkdir(parents=True, exist_ok=True)

    # Sauvegarder le dataset complet (raw)
    raw_path = raw_dir / "iris.csv"
    df.to_csv(raw_path, index=False)
    logger.info(f"ğŸ’¾ Dataset brut sauvegardÃ© dans : {raw_path}")

    # Diviser en train/test avec les paramÃ¨tres validÃ©s
    train_df, test_df = train_test_split(
        df, test_size=test_size, random_state=random_state, stratify=df["target"]
    )

    # Sauvegarder train et test
    train_path = processed_dir / "train.csv"
    test_path = processed_dir / "test.csv"

    train_df.to_csv(train_path, index=False)
    test_df.to_csv(test_path, index=False)

    logger.info(f"ğŸ’¾ Dataset d'entraÃ®nement sauvegardÃ© dans : {train_path}")
    logger.info(f"ğŸ’¾ Dataset de test sauvegardÃ© dans : {test_path}")
    logger.info(f"   Train: {len(train_df)} Ã©chantillons")
    logger.info(f"   Test: {len(test_df)} Ã©chantillons")

    # Statistiques
    logger.info("\nğŸ“Š Statistiques du dataset :")
    logger.info(f"   Total: {len(df)} Ã©chantillons")
    logger.info(f"   Features: {len(iris.feature_names)}")
    logger.info(f"   Classes: {len(iris.target_names)}")
    logger.info(f"   Distribution des classes (train):")
    logger.info(train_df["target_name"].value_counts().to_string())

    logger.info("âœ… PrÃ©paration des donnÃ©es terminÃ©e !")
    return train_path, test_path


if __name__ == "__main__":
    prepare_iris_data()
