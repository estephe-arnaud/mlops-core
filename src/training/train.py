"""
Module d'entra√Ænement du mod√®le ML
Mod√®le de classification sur le dataset Iris
Int√©gration MLflow pour le tracking des exp√©riences (Semaine 4)
Lit les param√®tres depuis params.yaml avec validation Pydantic
"""

import json
import logging
from pathlib import Path
from typing import Optional, Tuple

import joblib
import mlflow
import mlflow.sklearn
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

from src.config import get_config
from src.evaluation.evaluate import evaluate_model

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def load_data(
    test_size: float, random_state: int
) -> Tuple[pd.DataFrame, pd.DataFrame, dict]:
    """
    Charge les donn√©es depuis CSV (DVC pipeline) ou scikit-learn
    
    Returns:
        Tuple[train_df, test_df, iris_metadata]
    """
    train_path = Path("data/processed/train.csv")
    test_path = Path("data/processed/test.csv")
    
    if train_path.exists() and test_path.exists():
        logger.info("   üìÇ Chargement depuis les fichiers CSV (DVC pipeline)...")
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        iris = load_iris()  # Pour les m√©tadonn√©es
        return train_df, test_df, {
            "feature_names": iris.feature_names.tolist(),
            "target_names": iris.target_names.tolist(),
        }
    else:
        logger.info("   üì¶ Chargement depuis scikit-learn...")
        iris = load_iris()
        df = pd.DataFrame(iris.data, columns=iris.feature_names)
        df["target"] = iris.target
        
        train_df, test_df = train_test_split(
            df, test_size=test_size, random_state=random_state, stratify=df["target"]
        )
        
        return train_df, test_df, {
            "feature_names": iris.feature_names.tolist(),
            "target_names": iris.target_names.tolist(),
        }


def train_model(
    n_estimators: Optional[int] = None,
    max_depth: Optional[int] = None,
    random_state: Optional[int] = None,
    test_size: Optional[float] = None,
    use_mlflow: bool = True,
) -> Tuple[RandomForestClassifier, dict]:
    """
    Entra√Æne un mod√®le RandomForest sur le dataset Iris avec tracking MLflow
    Les param√®tres sont lus depuis params.yaml avec validation Pydantic si non fournis

    Args:
        n_estimators: Nombre d'arbres dans la for√™t (surcharge params.yaml si fourni)
        max_depth: Profondeur maximale des arbres (surcharge params.yaml si fourni)
        random_state: Graine al√©atoire pour la reproductibilit√© (surcharge params.yaml si fourni)
        test_size: Proportion du dataset pour le test (surcharge params.yaml si fourni)
        use_mlflow: Activer le tracking MLflow

    Returns:
        Tuple[RandomForestClassifier, dict]: Mod√®le entra√Æn√© et m√©tadonn√©es
    """
    config = get_config()
    n_estimators = n_estimators if n_estimators is not None else config.train.n_estimators
    max_depth = max_depth if max_depth is not None else config.train.max_depth
    random_state = random_state if random_state is not None else config.train.random_state
    test_size = test_size if test_size is not None else config.train.test_size

    # Configuration MLflow
    if use_mlflow:
        mlflow.set_experiment("iris-classification")
        mlflow.start_run()

    logger.info("üå± Chargement du dataset Iris...")
    train_df, test_df, iris_metadata = load_data(test_size, random_state)
    
    # S√©parer features et target
    feature_cols = ["sepal length (cm)", "sepal width (cm)", 
                   "petal length (cm)", "petal width (cm)"]
    X_train = train_df[feature_cols].values
    y_train = train_df["target"].values
    X_test = test_df[feature_cols].values
    y_test = test_df["target"].values

    # Hyperparam√®tres
    hyperparams = {
        "n_estimators": n_estimators,
        "max_depth": max_depth if max_depth else "None",
        "random_state": random_state,
        "test_size": test_size,
    }

    # Calculer les dimensions (pour MLflow et m√©tadonn√©es)
    n_features = X_train.shape[1]
    n_samples = len(X_train) + len(X_test)

    # Logger les hyperparam√®tres dans MLflow
    if use_mlflow:
        mlflow.log_params(hyperparams)
        mlflow.log_param("algorithm", "RandomForestClassifier")
        mlflow.log_param("dataset", "Iris")
        mlflow.log_param("n_features", n_features)
        mlflow.log_param("n_samples", n_samples)
        mlflow.log_param("n_classes", len(iris_metadata["target_names"]))

    logger.info("ü§ñ Entra√Ænement du mod√®le RandomForest...")
    logger.info(f"   Hyperparam√®tres: {hyperparams}")
    model = RandomForestClassifier(
        n_estimators=n_estimators, max_depth=max_depth, random_state=random_state
    )
    model.fit(X_train, y_train)

    # √âvaluation
    metrics, metadata = evaluate_model(
        model, X_test, y_test, iris_metadata, use_mlflow=use_mlflow
    )

    # Sauvegarde du mod√®le (m√©thode classique)
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)
    model_path = models_dir / "iris_model.pkl"
    joblib.dump(model, model_path)
    logger.info(f"üíæ Mod√®le sauvegard√© dans : {model_path}")

    # Sauvegarde via MLflow
    if use_mlflow:
        mlflow.sklearn.log_model(
            model, "model", registered_model_name="IrisClassifier"
        )
        logger.info("üìä Mod√®le enregistr√© dans MLflow")

    # Sauvegarde des m√©tadonn√©es
    metadata.update({
        "model_type": "RandomForestClassifier",
        "n_estimators": n_estimators,
        "max_depth": max_depth,
        "random_state": random_state,
        "n_features": n_features,
        "n_samples": n_samples,
    })

    metadata_path = models_dir / "model_metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2)

    # Logger les m√©tadonn√©es dans MLflow
    if use_mlflow:
        mlflow.log_dict(metadata, "model_metadata.json")
        mlflow.end_run()
        logger.info(f"üîó MLflow UI: mlflow ui (http://localhost:5000)")

    logger.info("‚úÖ Entra√Ænement termin√© avec succ√®s !")
    return model, metadata


if __name__ == "__main__":
    # Les param√®tres seront automatiquement lus depuis params.yaml avec validation
    train_model()

